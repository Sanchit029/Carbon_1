# System Overview

## Problem We're Solving

```
Clients send events:
âŒ No strict schema          â†’ Solution: Normalization layer
âŒ Format changes            â†’ Solution: Configurable mappings  
âŒ May resend events         â†’ Solution: Idempotency keys
âŒ May fail mid-request      â†’ Solution: Partial failure handling
âŒ Duplicate processing risk â†’ Solution: Deduplication
```

## Solution Architecture

```
Raw Events
    â†“
[Ingest] â†’ Store raw data (audit trail)
    â†“
[Normalize] â†’ Convert to canonical form
    â†“
[Deduplicate] â†’ Check if seen before
    â”œâ”€ YES â†’ Return existing ID (no reprocessing)
    â””â”€ NO  â†’ Continue
    â†“
[Store] â†’ Save to processed_events
    â†“
[Aggregation] â†’ Query for insights
    â†“
[API/UI] â†’ Expose results
```

## Key Innovation: Idempotency Key

Without unique event IDs from clients, we create our own:

```
Idempotency Key = Hash(client_id + metric + amount + timestamp_minute)

Result: Same event retried = same key = no double-processing
```

## Data Structure

```
raw_events (audit trail)
    â†“
    â”œâ”€â†’ passed normalization?
    â”‚       â”œâ”€ YES â†’ processed_events (canonical form)
    â”‚       â”‚       â”œâ”€â†’ idempotency_keys (dedup lookup)
    â”‚       â”‚       â””â”€â†’ Aggregation queries âœ“
    â”‚       â””â”€ NO  â†’ failed_events (with error)
    â”‚
    â””â”€ Used for: Debugging, understanding what went wrong
```

## Failure Handling

### Case 1: Event Already Processed (Duplicate)
```
Request 1 â†’ Processed âœ“ â†’ DB record created
Request 2 â†’ Check key â†’ Found in idempotency_keys
            â†’ Return same ID
            â†’ No reprocessing âœ“
            â†’ Count doesn't increase âœ“
```

### Case 2: Database Fails During Write
```
Request â†’ Normalize OK â†’ Try to INSERT
          âŒ DB fails
          â†’ Return HTTP 500
          â†’ Client can retry
          â†’ On retry: key not found (write failed)
          â†’ Try again (safe to retry)
```

### Case 3: Validation Fails
```
Request â†’ Normalization fails (invalid amount)
          â†’ Record in failed_events
          â†’ Return HTTP 400
          â†’ Not counted in aggregation âœ“
          â†’ Error reason stored for debugging
```

## What Gets Counted

```
Aggregation counts ONLY:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ processed_events â”‚  â† Canonical form
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†‘
        â”‚
   Never raw_events
     (would be wrong)
```

## Idempotency In Action

```
SCENARIO: Same event sent 3 times

Message 1: Send event X (amount: $100, client: A, metric: sales)
           â†’ Generate key: "A-1704067200000-hash123"
           â†’ Not in idempotency_keys
           â†’ INSERT into processed_events
           â†’ Record key in idempotency_keys
           â†’ Response: {success: true, processedEventId: 42}

Message 2: Network lost, client retries same event
           â†’ Generate key: "A-1704067200000-hash123" (SAME!)
           â†’ Check idempotency_keys: FOUND!
           â†’ Return: {success: true, isDuplicate: true, processedEventId: 42}
           â†’ No INSERT (no double-counting)

Message 3: Client retries again after failure
           â†’ Generate key: "A-1704067200000-hash123" (SAME!)
           â†’ Check idempotency_keys: FOUND!
           â†’ Return: {success: true, isDuplicate: true, processedEventId: 42}

RESULT: All 3 requests return successfully, but only 1 entry in database âœ“
```

## Database Consistency Guarantees

```
Level 1: Idempotency Check
    â””â”€ Prevents unnecessary reprocessing

Level 2: UNIQUE Constraint
    â””â”€ processed_events.idempotency_key UNIQUE
    â””â”€ If level 1 fails, this catches it

Level 3: Audit Trail
    â””â”€ raw_events stores all input
    â””â”€ Can verify correctness later

Result: No data corruption even with failures âœ“
```

## Client Format Variations Handled

```
Client A:
{
  "source": "client_A",
  "payload": {
    "metric": "transaction",
    "amount": "1200",
    "timestamp": "2024/01/01"
  }
}
        â†“ Different fields, different structure
        â†“ Normalization handles both!
Client B:
{
  "client": "client_B",
  "event_type": "payment",
  "value": 1200,
  "event_time": "2024-01-01T00:00:00Z"
}

Both end up as:
{
  "client_id": "...",
  "metric": "...",
  "amount": 1200,           â† Always a number
  "timestamp": "2024-01-01T00:00:00Z"  â† Always ISO
}
```

## Statistics Computed

```
From processed_events:

For each client:
â”œâ”€ count: How many events
â”œâ”€ total: Sum of amounts
â”œâ”€ average: total / count

System-wide:
â”œâ”€ totalProcessed: Count of successful events
â”œâ”€ totalFailed: Count of validation failures
â”œâ”€ totalAmount: Sum of all amounts
â””â”€ successRate: Percentage processed successfully
```

## API Endpoints

| Endpoint | Method | Purpose |
|----------|--------|---------|
| /api/events | POST | Ingest raw event |
| /api/aggregate | GET | Aggregated statistics |
| /api/events | GET | List processed events |
| /api/failed | GET | Debug failed events |
| /api/summary | GET | System statistics |

## Files & Their Role

```
database.js       â†’ SQLite schema & initialization
normalization.js  â†’ Convert any format to canonical
deduplication.js  â†’ Idempotency key generation & checking
processor.js      â†’ Orchestrate the 7-step pipeline
aggregation.js    â†’ Query interface for analytics
server.js         â†’ REST API (routes & responses)
frontend/index.html â†’ User interface (submit & view)
```

## Limitations & Future Work

```
Current:
âœ… Simple, correct, easy to understand
âœ… Good for hundreds of events/day
âœ… Single process, SQLite

Doesn't scale to:
âŒ Thousands of req/sec (SQLite bottleneck)
âŒ Terabytes of data (single database)
âŒ Multiple servers (no replication)

To scale:
â†’ PostgreSQL (distributed, transactions)
â†’ Redis (cache, idempotency cache)
â†’ Message queue (async, resilience)
â†’ Horizontal scaling (load balancer)
```

## Success Criteria Met

âœ… Handles unreliable clients (schema variations, missing fields)  
âœ… Prevents duplicate processing (idempotency keys)  
âœ… Safe against partial failures (UNIQUE constraints, audit trail)  
âœ… Consistent aggregations (queries canonical form only)  
âœ… Extensible (add clients without code changes)  
âœ… Clear error messages (validation vs system errors)  
âœ… Auditable (stores raw events)  
âœ… Testable (UI + API)  

## Next Steps to Use

1. `npm install`
2. `npm start`
3. Open http://localhost:3001
4. Try "Sample: Client A"
5. Click "Submit Event"
6. Check "Aggregation" tab
7. Try duplicate detection (submit same event twice)
8. Check "Simulate Database Failure" to test error handling

**That's the whole system!** ğŸš€

---

For deep dives, see:
- **README.md** - Full documentation & design decisions
- **ARCHITECTURE.md** - System design diagrams & explanations
- **TESTING.md** - How to test every feature
- **QUICKSTART.md** - Getting started in 5 minutes
